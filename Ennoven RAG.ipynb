{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f8a93-6bb4-4146-bc5b-99d8aee8c2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "!pip install pypdf langchain langchain-community langchain-core langchain-openai chromadb python-dotenv\n",
    "\n",
    "# Usar solo en Databricks\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee64ef-1c0d-491a-a8be-2df4028a4179",
   "metadata": {},
   "source": [
    "# Ejercicio de Ingeniería de Software con Inteligencia Artificial\n",
    "\n",
    "## Descripción de la Prueba\n",
    "\n",
    "El objetivo de esta prueba es evaluar tus habilidades y conocimientos en el desarrollo de software con inteligencia artificial, específicamente en sistemas RAG (Retrieval-Augmented Generation), embeddings, y bases de datos vectoriales.\n",
    "\n",
    "Se te proporcionarán tres archivos PDF con reportes de resultados de Grupo Bimbo, así como las llaves de API y los endpoints de OpenAI. Tu tarea será utilizar estos recursos para desarrollar un sistema que pueda realizar tareas de retrieval de manera eficiente.\n",
    "\n",
    "## Objetivos de la Prueba\n",
    "\n",
    "1. Comprensión de los datos: Deberás ser capaz de procesar y entender los datos proporcionados en los archivos PDF.\n",
    "\n",
    "2. Uso de OpenAI y embeddings: Deberás demostrar tu habilidad para trabajar con OpenAI y embeddings para realizar tareas de retrieval.\n",
    "\n",
    "3. Implementación de bases de datos vectoriales: Deberás implementar una base de datos vectorial para manejar eficientemente los datos.\n",
    "\n",
    "4. Desarrollo de un sistema RAG: Finalmente, deberás demostrar tu capacidad para desarrollar un sistema RAG que pueda realizar tareas de retrieval y generación de texto de manera eficiente.\n",
    "\n",
    "## Presentación del Código\n",
    "\n",
    "Tu código debe seguir las mejores prácticas de codificación y puede presentarse en notebooks de Jupyter. Esto incluye:\n",
    "\n",
    "· Legibilidad: El código debe ser fácil de leer y entender.\n",
    "\n",
    "· Comentarios: Debes incluir comentarios que expliquen tu razonamiento y las decisiones de diseño que tomaste.\n",
    "\n",
    "· Estructura: El código debe estar bien estructurado y organizado.\n",
    "\n",
    "· Pruebas: Debes incluir pruebas para asegurar que tu código funciona como se espera.\n",
    "\n",
    "Para revisión de este proyecto ser realizará una sesión en línea donde presentarás tu abordaje el problema, tu resolución técnica y código generado.\n",
    "\n",
    "- APIkey: 75433f0e2ce040ebb90a2fa457fbc815\n",
    "- EndPoint: https://ennovenoai.openai.azure.com/\n",
    "- Modelo Openai: EnnovenGPTTurbo\n",
    "- Modelo Ada-002: EnnovenAda\n",
    "\n",
    "Anexo igualmente los PDFs que sirven como fuente de datos para el sistema RAG, recordando que buscamos una búsqueda enriquecida: (Datos + metadata)\n",
    "\n",
    "El archivo contiene tablas, e imágenes, así como textos. Se espera del ejercicio el procesamiento de las tablas e imágenes dentro del modelo. Así como una aproximación de reranking de los resultados para un mejor contexto de respuesta.\n",
    "\n",
    "Las API Keys serán cambiadas una ves terminado el ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19588d92-68d9-47b0-a4c4-7c5860165125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bibliotecas para interactuar con el LLM\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# Para generar los embeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Para cargar los datos en formato PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Para el splitting de los textos\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Para almacenar los embeddings en ChromaDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# Para generar el RAG usando la base de datos vectorial\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Interactuar con el sistema de archivos\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de ambiente desde el .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6f941-767f-4ced-af2d-2a10da1d9642",
   "metadata": {},
   "source": [
    "## Credenciales de accesso\n",
    "\n",
    "Para Databricks: Las credenciales de acceso al modelo han sido almacenadas en variables de ambiente en la configuración del Clúster para mayor seguridad.\n",
    "\n",
    "<img src=\"./img/databricks-env-variables.png\">\n",
    "\n",
    "Para entornos de prueba local: Las credenciales se encuentran en un archivo `.env`\n",
    "\n",
    "<img src=\"./img/variables-de-ambiente.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311ce685-4ec3-4c1f-bf52-e1b4e35d3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "azure_deployment = os.environ[\"AZURE_DEPLOYMENT\"]\n",
    "azure_openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a26845-ecbe-4319-8c40-b48b095bd253",
   "metadata": {},
   "source": [
    "Creamos una instancia para usar el modelo de ChatGPT de Azure y el modelo para generar los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7f7c2d-986a-4656-ab87-bf85f124c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una conexión con el modelo GPT de Azure\n",
    "model = AzureChatOpenAI(\n",
    "    api_key = azure_openai_api_key, # API Key\n",
    "    azure_endpoint = azure_openai_endpoint, # Endpoint\n",
    "    openai_api_version = openai_api_version, # Obtenido de la documentación\n",
    "    model_name = model_name, # Nombre del modelo\n",
    "    temperature = 0 # Temperatura del modelo GPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739c7577-6aae-4e13-bd18-1f07aa89f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo para generar los embeddings\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    api_key = azure_openai_api_key, # API Key\n",
    "    azure_endpoint = azure_openai_endpoint, # Endpoint\n",
    "    azure_deployment = azure_deployment, # Nombre del modelo de embeddings\n",
    "    openai_api_version = openai_api_version # Obtenido de la documentación\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b212a4",
   "metadata": {},
   "source": [
    "# Cargar los datos para el RAG\n",
    "\n",
    "Las fuentes de datos pueden ser variadas, pero para este ejercicio tenemos archivos en formato PDF dentro de directorio `data`.\n",
    "\n",
    "Podemos acceder a ese directorio y filtrar los archivos usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898e5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros para el splitting\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "# Inicializar el splitter con los hiperparametros\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size, \n",
    "    chunk_overlap = chunk_overlap,\n",
    "    separators = [\".\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c5305ea-56ea-4d15-81dc-eee7581050f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/Reporte_Definitivo_BMV_XBRL_Español_Mar_23.pdf\n",
      "Extracting text from file: data/Reporte_Definitivo_BMV_XBRL_Español_Mar_23.pdf\n",
      "Splitting docs for data/Reporte_Definitivo_BMV_XBRL_Español_Mar_23.pdf\n",
      "Creating embeddings for docs in data/Reporte_Definitivo_BMV_XBRL_Español_Mar_23.pdf\n",
      "Embeddings created\n",
      "\n",
      "Processing file: data/Reporte_Definitivo_BMV_XBRL_Español_Jun_23.pdf\n",
      "Extracting text from file: data/Reporte_Definitivo_BMV_XBRL_Español_Jun_23.pdf\n",
      "Splitting docs for data/Reporte_Definitivo_BMV_XBRL_Español_Jun_23.pdf\n",
      "Creating embeddings for docs in data/Reporte_Definitivo_BMV_XBRL_Español_Jun_23.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inicializar el splitter con los hiperparametros\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size, \n",
    "    chunk_overlap = chunk_overlap,\n",
    "    separators = [\".\"]\n",
    ")\n",
    "\n",
    "# Inicializar la base de datos vectorial\n",
    "vectordb = Chroma(\n",
    "    persist_directory = \"db\",\n",
    "    embedding_function = embedding_model,\n",
    "    collection_name = \"ennoven_rag\"\n",
    ")\n",
    "\n",
    "# Configurar Chroma para hacer datos persistentes en disco\n",
    "vectordb.persist()\n",
    "\n",
    "# Para cada PDF dentro del directorio\n",
    "# for file in os.listdir(\"data\"):\n",
    "for file in glob.glob(\"data/*.pdf\"):\n",
    "# Crear una conexión con el PDF\n",
    "    print(f\"Processing file: {file}\")\n",
    "    loader = PyPDFLoader(f\"{file}\")\n",
    "    \n",
    "    # Extraer el texto del PDF\n",
    "    print(f\"Extracting text from file: {file}\")\n",
    "    pdf_data = loader.load()\n",
    "    \n",
    "    # Dividir el contenido del PDF en documentos\n",
    "    print(f\"Splitting docs for {file}\")\n",
    "    docs = splitter.split_documents(pdf_data)\n",
    "    \n",
    "    # Crear los embeddings de los documentos usando el modelo y almacenarlos en ChromaDB\n",
    "    print(f\"Creating embeddings for docs in {file}\")\n",
    "    docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "    print(\"Embeddings created\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4e90a",
   "metadata": {},
   "source": [
    "Una vez almacenados los embeddings en la base de datos vectorias, podemos usarla para construir un RAG (básico) para obtener en base al contexto de los archivos PDF que extraímos.\n",
    "\n",
    "Usamos la función `RetrievalQAWithSourcesChain` para obtener una respuesta junto con su metadata, que incluye la fuente y la similitud. Especialmente útil para identificar si la respuesta generada por el modelo ha sido una alucinación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornar las referencias al RAG desde la base de datos vectorial de ChormaDB\n",
    "print(f\"Returning references to RAG data sources\")\n",
    "rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = model, # Usar el modelo GPT\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = docstorage.as_retriever() # Usar el contenido de la base de datos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec577ce8",
   "metadata": {},
   "source": [
    "Para probar este modelo primitivo, podemos usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c2bec-fdf7-4b3f-a0f0-146eccd2fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Cuales son los principales prestamistas bancarios de la compañia, con la que se mantiene un endeudamiento\"\n",
    "rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd05ee-c7a4-4030-955e-7dca09d23444",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Desglosa la deuda con los principales prestamistas bancarios de la compañia\"\n",
    "rag.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
